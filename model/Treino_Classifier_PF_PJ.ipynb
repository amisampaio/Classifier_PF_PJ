{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Treino_Classifier_PF_PJ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWqYrWbfcvLr"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import sklearn.feature_extraction.text\r\n",
        "import sklearn.naive_bayes\r\n",
        "import sklearn.metrics\r\n",
        "import sklearn.model_selection\r\n",
        "import sklearn.pipeline\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_611mRXu0Ix"
      },
      "source": [
        "import zipfile\r\n",
        "path = \"/content/\"\r\n",
        "file = \"Base de Treino Pessoas.zip\"\r\n",
        "with zipfile.ZipFile(path + \"Base de Treino Pessoas.zip\", 'r') as zip_ref:\r\n",
        "          zip_ref.extractall(path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3qkhW8ndq-S"
      },
      "source": [
        "# colocar cÃ³digo para pegar a base do hub de dados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfHg0kv_crWw"
      },
      "source": [
        "def PrepareData():\r\n",
        "  \"\"\"\r\n",
        "  Function to prepare the dataset for model\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  df = pd.read_csv(\"Base de Treino Pessoas.csv\")\r\n",
        "  df['TIPO_PESSOA'] = df['TIPO_PESSOA'].replace('PESSOA FISICA (CPF)' ,'PF')\r\n",
        "  df['TIPO_PESSOA'] = df['TIPO_PESSOA'].replace('PESSOA JURIDICA (CNPJ)' ,'PJ')\r\n",
        "  df['TIPO_PESSOA'] = df['TIPO_PESSOA'].replace('PESSOA JURIDICA (CNP' ,'PJ')\r\n",
        "  df= df[~df[\"TIPO_PESSOA\"].isna()]\r\n",
        "  df = df.drop_duplicates()\r\n",
        "  dinamica = df.groupby(['NOME_PESSOA']).size().reset_index().rename(columns={0:'contagem'})\r\n",
        "  remove_list = dinamica[dinamica[\"contagem\"]>1][\"NOME_PESSOA\"]\r\n",
        "  df = df[~df['NOME_PESSOA'].isin(remove_list)]\r\n",
        "\r\n",
        "  print(df[\"TIPO_PESSOA\"].unique())\r\n",
        "\r\n",
        "  Y = df[\"TIPO_PESSOA\"].values\r\n",
        "  X = df[\"NOME_PESSOA\"].values\r\n",
        "  \r\n",
        "  # handling dtype issuse, in NOME_PESSOA, there are numbers \r\n",
        "  x=[]\r\n",
        "  y=[]\r\n",
        "  for i, j in zip(X.tolist(),Y.tolist()):\r\n",
        "    if type(i)!=str:\r\n",
        "      continue \r\n",
        "    else:\r\n",
        "      x.append(i)\r\n",
        "      y.append(j)\r\n",
        "  x= np.array(x)\r\n",
        "  y = np.array(y)\r\n",
        "  return x, y\r\n",
        "\r\n",
        "def train_and_evaluate(train,ytrain):\r\n",
        "    \r\n",
        "    # Convert to bag of words\r\n",
        "    count_vect = sklearn.feature_extraction.text.CountVectorizer(strip_accents='ascii', stop_words='english', lowercase=True, ngram_range=(1,1))\r\n",
        "    X = count_vect.fit_transform(train)\r\n",
        "    # Convert from occurrences to frequencies\r\n",
        "    # Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\r\n",
        "    # To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\r\n",
        "    transformer = sklearn.feature_extraction.text.TfidfTransformer()\r\n",
        "    X = transformer.fit_transform(X)\r\n",
        "    # Create a model\r\n",
        "    model = sklearn.naive_bayes.MultinomialNB(alpha=0.3, fit_prior=True, class_prior=None)\r\n",
        "    # Train the model\r\n",
        "    model.fit(X, ytrain)\r\n",
        "    # Save models\r\n",
        "    joblib.dump(count_vect, 'vectorizer.jbl')\r\n",
        "    joblib.dump(transformer, 'transformer.jbl')\r\n",
        "    joblib.dump(model, 'model.jbl')\r\n",
        "    # Evaluate on training data\r\n",
        "    print('-- Training data --')\r\n",
        "    predictions = model.predict(X)\r\n",
        "    accuracy = sklearn.metrics.accuracy_score(ytrain, predictions)\r\n",
        "    print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\r\n",
        "    print('Classification Report:')\r\n",
        "    print(sklearn.metrics.classification_report(ytrain, predictions))\r\n",
        "    print('')\r\n",
        "    # Evaluate with 10-fold CV\r\n",
        "    print('-- 10-fold CV --')\r\n",
        "    predictions = sklearn.model_selection.cross_val_predict(model, X, ytrain, cv=10)\r\n",
        "    accuracy = sklearn.metrics.accuracy_score(ytrain, predictions)\r\n",
        "    print('Accuracy: {0:.2f}'.format(accuracy * 100.0))\r\n",
        "    print('Classification Report:')\r\n",
        "    print(sklearn.metrics.classification_report(ytrain, predictions))\r\n",
        "\r\n",
        "# The main entry point for this module\r\n",
        "def Start_Training():\r\n",
        "    # Train and evaluate\r\n",
        "    x,y = PrepareData()\r\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\r\n",
        "    train_and_evaluate(X_train,y_train)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dvpw1Fbq-bk",
        "outputId": "b4423a1c-f465-4050-b456-09ddf6dd6e4e"
      },
      "source": [
        "# If you want to train then uncomment these below line\r\n",
        "Start_Training()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PF' 'PJ']\n",
            "-- Training data --\n",
            "Accuracy: 99.89\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          PF       1.00      1.00      1.00   1738637\n",
            "          PJ       1.00      0.97      0.98     57303\n",
            "\n",
            "    accuracy                           1.00   1795940\n",
            "   macro avg       1.00      0.98      0.99   1795940\n",
            "weighted avg       1.00      1.00      1.00   1795940\n",
            "\n",
            "\n",
            "-- 10-fold CV --\n",
            "Accuracy: 99.80\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          PF       1.00      1.00      1.00   1738637\n",
            "          PJ       0.97      0.96      0.97     57303\n",
            "\n",
            "    accuracy                           1.00   1795940\n",
            "   macro avg       0.99      0.98      0.98   1795940\n",
            "weighted avg       1.00      1.00      1.00   1795940\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFM0_JKjteKW"
      },
      "source": [
        "file = open(\"Classifier_PF_PJ.py\", \"w\") \r\n",
        "file.write(\"\"\"\r\n",
        "import joblib\r\n",
        "def is_PF_PJ(texto):\r\n",
        "    path = \"Classifier_PF_PJ/\"\r\n",
        "    vectorizer = joblib.load(path + 'vectorizer.jbl')\r\n",
        "    transformer = joblib.load(path + 'transformer.jbl')\r\n",
        "    model = joblib.load(path + 'model.jbl')\r\n",
        "    X = vectorizer.transform([texto])\r\n",
        "    X = transformer.transform(X)\r\n",
        "    predictions = model.predict(X)\r\n",
        "    return predictions[0]\r\n",
        "\"\"\") \r\n",
        "file.close() "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3hPYrtBdlLn"
      },
      "source": [
        "import Classifier_PF_PJ"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pef5sM8kpVqt",
        "outputId": "c9e043ca-4d8c-47e1-c201-57ce437fd740"
      },
      "source": [
        "Classifier_PF_PJ.is_PF_PJ(\"Amilcar S Sampaio\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'PF'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4lCUdQMFpbYd",
        "outputId": "dccef3db-c0af-4b7a-cffd-e648be7c056c"
      },
      "source": [
        "Classifier_PF_PJ.is_PF_PJ(\"BCO BTG PACTUAL S.A.\") "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'PJ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}